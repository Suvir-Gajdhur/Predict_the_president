---
title: "Bag-of-words"
author: "Vaughn Saben"
date: "08 September 2018"
output: markdown_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## SECTION 0: DATA AND DATA PROCESSING

### a. Required Packages

```{r}
library(tidyverse)
library(tidytext)
library(dplyr)

library(stringr)
library(doParallel)

library(rpart)
```

### b. Read in data

```{r}
library(readtext)

speeches <- readtext("data/sona-text-1994-2018")
nspeeches <- nrow(speeches)

## Extract date information

# First newline character

date_all_index <- str_locate(speeches$text,"\n")[, 1]

library(lubridate)

dates <- foreach(i = 1:nspeeches, .combine = c) %do% {
   str_sub(speeches$text[i], start = 1, end = date_all_index[i] - 1)
}

# Add date information to table

library(dplyr)

speeches <- speeches %>% mutate(date = dmy(dates))

## Remove: date information AND line breaks from speech text

speeches$text <- foreach(i = 1:nspeeches, .combine = c) %do% {
  speeches$text[i] <- str_sub(speeches$text[i], start = date_all_index[i])
  speeches$text[i] <- str_replace_all(speeches$text[i], "[\r\n]", " ")
}

## Add: President

president_id <- unlist(str_extract_all(speeches$doc_id, "[A-Z][a-z]+"))

speeches$president_id <- president_id

```

### c. Tokenisation (sentences)

```{r}
## Sentences

tidy_sen <- unnest_tokens(speeches, 
                          sentences, 
                          text, 
                          token = "sentences")
tidy_sen <- mutate(tidy_sen, sen_id = 1:nrow(tidy_sen))
```

### d. Convert into bag-of-words form

```{r}

# Word tokenisation

tidy_word <- unnest_tokens(tidy_sen, 
                           word, 
                           sentences, 
                           token = "words") 

# Inverse term frequency weightings

sentence_tdf <- tidy_word %>% 
  group_by(sen_id, word) %>%
  count() %>%
  group_by(sen_id) %>%
  mutate(total = sum(n)) %>%
  ungroup() %>% 
  bind_tf_idf(word, sen_id, n)

# Bag of words

bag_of_words <- sentence_tdf %>% 
  select(sen_id, word, tf_idf) %>% 
  spread(key = word, value = tf_idf, fill = 0) %>% 
  left_join(select(tidy_sen, sen_id, president_id), by = "sen_id") %>%
  select(sen_id, president_id, everything())
  
nrow(bag_of_words) # Number of documents
ncol(bag_of_words) # Number of variables (words, plus id and response)
```

### e. Split sentence data into training (70%) and test (30%) sets

```{r}

set.seed(11)

# Training

training_sen_id <- bag_of_words %>%
  group_by(president_id) %>%
  sample_frac(0.7) %>%
  ungroup() %>%
  select(sen_id) %>%
  unlist()

training_sen <- subset(bag_of_words, unlist(sen_id) %in% training_sen_id) %>% 
  select(-sen_id)

# Testing 

testing_sen <- subset(bag_of_words, !(unlist(sen_id) %in% training_sen_id)) %>% 
  select(-sen_id)

```

### f. Fit classification tree

```{r}

## Classification tree

rpart_fit <- rpart(factor(president_id) ~ ., training_sen)

# Plot

options(repr.plot.width = 8, repr.plot.height = 10)
plot(rpart_fit, main = "Full Classification Tree")
text(rpart_fit, use.n = T, all = T, cex = .8)

# Accuracy

# > Training

rpart_fitTrain <- predict(rpart_fit, type = "class")
rpart_predTrain <- table(training_sen$president_id, rpart_fitTrain)
rpart_predTrain

sum(diag(rpart_predTrain))/(sum(rpart_predTrain)) # Training accuracy

# > Test

rpart_fitTest <- predict(rpart_fit, newdata = testing_sen, type = "class")
rpart_predTest <- table(testing_sen$president_id, rpart_fitTest)
rpart_predTest

sum(diag(rpart_predTest))/sum(rpart_predTest) # Test accuracy

## Random forest

library(randomForest)

rf_names <- str_extract(names(training_sen),"\\b[a-zA-Z'?]+\\b")
rf_word_bag <- rf_names[!is.na(rf_names)] 

rf_fit <- randomForest(
  as.factor(president_id) ~ rf_word_bag, 
  data = training_sen, 
  ntree = 1000, 
  importance = T, 
  do.trace = 100, 
  xtest = testing_sen[-president_id], 
  ytest = as.factor(testing_sen[president_id]))

```






#### i. Plot: Most commonly used words

```{r}

# > Absolute

tidy_word %>%
  group_by(president) %>%
  count(word, sort = TRUE) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(word, n), n, fill = president)) + geom_col() + coord_flip() + labs(x = "")

# > Relative

total_speeches <- tidy_speeches_word %>% 
  group_by(president) %>% 
  summarise(total = n())

tidy_speeches_word %>%
  group_by(president) %>% 
  count(word, sort = TRUE) %>%                              # Count: Number of times word used 
  left_join(total_speeches) %>%                             # Add: Total speeches for each president
  mutate(freq = n/total) %>%                                # Add relative frequencies
  filter(rank(desc(freq)) < 20) %>%
  ggplot(aes(reorder(word, freq), freq, fill = president)) + 
  geom_col() + 
  coord_flip() + 
    xlab("") +
  facet_grid(.~president)

## Bigrams

tidy_speeches_bigrams <- speeches %>%
  # mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# a. Separate the bigrams 

library(tidyr)

bigrams_separated <- tidy_speech_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# b. Remove stop words

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# c. Rejoin bigrams

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# Plot: Most common bigrams 

bigrams_united %>% 
  group_by(president) %>%
  count(bigram, sort = TRUE) %>% 
  left_join(total_speeches) %>%                             # Add: Total speeches for each president
  mutate(freq = n/total) %>%                                # Add relative frequencies
  filter(rank(desc(freq)) < 20) %>%
  ggplot(aes(reorder(bigram, freq), freq, fill = president)) + 
  geom_col() + 
  coord_flip() + 
    xlab("") +
  facet_grid(.~president)

```

## 1: Bag-of-words

```{r}
word_bag <- tidy_speeches_word %>%
  group_by(word) %>%
  count() %>%
  ungroup() %>%
  top_n(2000, wt = n) %>%
  select(-n)

nrow(word_bag)

speeches_tdf <- tidy_speeches_word %>%
  inner_join(word_bag) %>%
  group_by(doc_id, word) %>%
  count() %>%  
  group_by(doc_id) %>%
  mutate(total = sum(n)) %>%
  ungroup()

# tdf (excl. stop words)

speeches_tdf <- speeches_tdf %>% 
  bind_tf_idf(word, doc_id, n)

bag_of_words <- speeches_tdf %>% 
  select(doc_id, word, tf_idf) %>% 
  spread(key = word, value = tf_idf, fill = 0) %>%
  left_join(select(speeches, doc_id, president), by = "doc_id") %>%
  select(doc_id, president.y, everything())

# Number of words

ncol(bag_of_words) - 2

table(bag_of_words$president.y) # No lost

## Building a bag of words classifier

set.seed(11)
training_ids <- bag_of_words %>% 
  group_by(president.y) %>% 
  sample_frac(0.7) %>% 
  ungroup() %>%
  select(doc_id)

training_speeches <- bag_of_words %>% 
  right_join(training_ids, by = "doc_id") %>%
  select(-doc_id)

test_speeches <- bag_of_words %>% 
  anti_join(training_ids, by = "doc_id") %>%
  select(-doc_id)

# Fit

library(rpart)

fit <- rpart(factor(president.y) ~ ., training_speeches)

# Plot

options(repr.plot.width = 8, repr.plot.height = 10)
plot(fit, main = "Full Classification Tree")
text(fit, use.n = TRUE, all = TRUE, cex=.8)


```

### c. Data properties

```{r}

## Time: First, last AND time range of speech data

min(speeches$date) # Earliest
max(speeches$date) # Latest

max(speeches$date) - min(speeches$date) # Time range

# Plot:

library(ggplot2)

ggplot(speeches, aes(x = date, fill = president)) +
  geom_histogram(position = "identity", 
                 bins = 30, 
                 show.legend = T) + 
  labs(x = "Date", 
       y = "Count") 

## Number of speeches by each president 

table(speeches$president)
                
```


## 2: Neural networks

```{r}

library(tensorflow)
library(keras)
library(modelr)

model <- keras_model_sequential()

model %>%
  
  layer_dense(units = 512, activation = 'relu', input_shape = c(8793)) %>%
  
  # embedding layer maps vocab indices into embedding_dims dimensions
  # layer_embedding(input_dim = vocab_size, output_dim = embedding_dims, 
  #                input_length = maxlen) %>%
  # add some dropout
  
  layer_dropout(0.2) %>%
  
  # convolutional layer
  
  # layer_conv_1d(
  #   filters = 250,
  #   kernel_size = 3,
  #   padding = "valid",  # "valid" means no padding, as we did it already
  #   activation = "relu",
  #   strides = 1
  # ) %>%
  # layer_global_max_pooling_1d() %>%
  
  layer_dense(128) %>%
  layer_dropout(0.2) %>%
  layer_activation("relu") %>%
  layer_dense(6) %>%   # six unit output layer
  layer_activation("softmax")

  
# model <- keras_model_sequential() %>% 
#     layer_dense(units = 16, activation = "relu", input_shape = c(maxlen)) %>% 
#     layer_dense(units = 16, activation = "relu") %>% 
#     layer_dense(units = 6, activation = "softmax")  

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

arow <- which(apply(x_train, 1, function(x){ sum(x) == 0}))
acol <- which(apply(x_train, 2, function(x){ sum(x) == 0}))

x_train <- as.matrix(as.data.frame(training_sen[, rf_word_bag]))[-arow, -acol]
y_train <- model_matrix(training_sen[-arow, ],  ~ president_id - 1)

ncol(x_train)

model %>%
  fit(
    x_train, y_train,
    batch_size = 32,
    epochs = 30,
    validation_split = 0.2,
    callbacks = list(
      callback_early_stopping(patience = 5),
      callback_reduce_lr_on_plateau(patience = 3)
    )
  )

```





















