---
title: "Bag-of-words"
author: "Vaughn Saben"
date: "08 September 2018"
output: markdown_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## SECTION 0: DATA AND DATA PROCESSING

### a. Required Packages

```{r}
library(tidyverse)
library(tidytext)
library(dplyr)

library(stringr)
library(doParallel)

library(rpart)
```

### b. Read in data

```{r}
library(readtext)

speeches <- readtext("data/sona-text-1994-2018")
nspeeches <- nrow(speeches)

## Extract date information

# First newline character

date_all_index <- str_locate(speeches$text,"\n")[, 1]

library(lubridate)

dates <- foreach(i = 1:nspeeches, .combine = c) %do% {
   str_sub(speeches$text[i], start = 1, end = date_all_index[i] - 1)
}

# Add date information to table

library(dplyr)

speeches <- speeches %>% mutate(date = dmy(dates))

## Remove: date information, line breaks and unwanted characters from speech text

speeches$text <- foreach(i = 1:nspeeches, .combine = c) %do% {
  speeches$text[i] <- str_sub(speeches$text[i], start = date_all_index[i])
  speeches$text[i] <- str_replace_all(speeches$text[i], "[\r\n]", " ")
}

## Add: President

president_id <- unlist(str_extract_all(speeches$doc_id, "[A-Z][a-z]+"))

speeches$president_id <- president_id

```

### c. Tokenisation (sentences)

```{r}
## Sentences

tidy_sen <- unnest_tokens(speeches, 
                          sentences, 
                          text, 
                          token = "sentences")
tidy_sen <- mutate(tidy_sen, sen_id = 1:nrow(tidy_sen))
```

### d. Convert into bag-of-words form

```{r}
# Word tokenisation

tidy_word <- unnest_tokens(tidy_sen, 
                           word, 
                           sentences, 
                           token = "words") 

# Inverse term frequency weightings

sentence_tdf <- tidy_word %>% 
  group_by(sen_id, word) %>%
  count() %>%
  group_by(sen_id) %>%
  mutate(total = sum(n)) %>%
  ungroup() %>% 
  bind_tf_idf(word, sen_id, n)

# Bag of words

bag_of_words <- sentence_tdf %>% 
  select(sen_id, word, tf_idf) %>% 
  spread(key = word, value = tf_idf, fill = 0) %>% 
  left_join(select(tidy_sen, sen_id, president_id), by = "sen_id") %>%
  select(sen_id, president_id, everything())
  
nrow(bag_of_words) # Number of documents
ncol(bag_of_words) # Number of variables (words, plus id and response)
```

## SECTION 1: Build a bag-of-words classifier 

In addition to the neural network classifier, a classification tree was built to provide a benchmark performance measure. 2 approaches to the classification splitting procedure priors are investigated: 

1. Default classification priors: Each president's prior probability is a proportional to the sentence counts 
2. Equal classification priors: Each president's prior probability is equal to 1/6.

The sentence data is split into training (70%) and test (30%) sets. The minimum number of observations in a node, for a split to be considered, is set to 10 to enable all presidents to be possible classification outputs. The minimum number of observation permitted in a terminal is also lowered for the same reason.

```{r, echo = F, eval = T}
# 1: Split sentence data into training (70%) and test (30%) sets

set.seed(11)

# Training

training_sen_id <- bag_of_words %>%
  group_by(president_id) %>%
  sample_frac(0.7) %>%
  ungroup() %>%
  select(sen_id) %>%
  unlist()

training_sen <- subset(bag_of_words, unlist(sen_id) %in% training_sen_id) %>% 
  select(-sen_id)

# Testing 

testing_sen <- subset(bag_of_words, !(unlist(sen_id) %in% training_sen_id)) %>% 
  select(-sen_id)

# 2: Fit classification tree

rpart_fit_eqp <- rpart(factor(president_id) ~ ., 
                       data = training_sen, 
                       method = "class", 
                       parms = list(prior = rep(1/6, 6)),
                       control = rpart.control(minsplit = 10, 
                                               minbucket = 5))

rpart_fit_neqp <- rpart(factor(president_id) ~ ., 
                        data = training_sen, 
                        method = "class", 
                        control = rpart.control(minsplit = 10, 
                                                minbucket = 5))

# Min split: Minimum number of observations that must exist in a node in order for a split to be attempted
# Min bucket: Minimum number of observations in a terminal

# Parms: 
# > Default priors for classification spitting: proportional to data counts 
# > Heavily favour Mbeki and Zuma (i.e. the presidents with the longest terms)
# > Adjust equal prior probabilities

```

#### Results

**Classification tree - default prior configuration**: The classification tree classifies *Mbeki* and *Zuma* only. Some classifications 

```{r}
# Frequency weighted priors

options(repr.plot.width = 8, repr.plot.height = 10)
plot(rpart_fit_neqp, main = "Full Classification Tree")
text(rpart_fit_neqp, use.n = T, all = T, cex = .7)

# > Only Mbeki and Zuma may be classified (longer president terms)
# > Some classification based on sentence constructors, especially first split

# Equal priors

options(repr.plot.width = 8, repr.plot.height = 10)
plot(rpart_fit_eqp, main = "Full Classification Tree")
text(rpart_fit_eqp, use.n = T, all = T, cex = .7)

# > More presidents may be classified
# > Single branch has many levels
# > Classification based on both sentence constructors and insightful words 



## Accuracy

# Function: Training

rpart_train_accuracy <- function(fit, data){
  rpart_fit_train <- predict(fit, type = "class")
  rpart_pred_train <- table(data$president_id, rpart_fit_train)
  accuracy = sum(diag(rpart_pred_train))/(sum(rpart_pred_train))
  list("Predictions" = rpart_pred_train, 
       "Accuracy" = accuracy)
}

# Output: Training

rpart_train_accuracy_eqp <- rpart_train_accuracy(rpart_fit_eqp, training_sen)

# > Mandela never predicted
# > Many predictioon errors
# > Unbalanced number of sentences associated with each president:
#   - More than half correctly predicted: Klerk, Motlanthe
# > Low accuracy: < 0.2 >> Poor model

rpart_train_accuracy_neqp <- rpart_train_accuracy(rpart_fit_neqp, training_sen) 

# > Mbeki and Zuma predicted only
# > Mbeki speeches are predicted more IF Mbeki (and v.v.)
# > Higher accuracy: Approximately 0.5 >> Need more levels

# Function: Testing

rpart_test_accuracy <- function(fit, data){
  rpart_fit_test <- predict(fit, newdata = data, type = "class")
  rpart_pred_test <- table(data$president_id, rpart_fit_test)
  accuracy = sum(diag(rpart_pred_test))/(sum(rpart_pred_test))
  list("Predictions" = rpart_pred_test, 
       "Accuracy" = accuracy)
}

# Output: Testing

rpart_test_accuracy_eqp <- rpart_test_accuracy(rpart_fit_eqp, testing_sen)

# > Performs worse than on training set (accuracy = 0.17)
# > Greatest frequency predicted correctly: Klerk, Motlanthe 

rpart_test_accuracy_neqp <- rpart_test_accuracy(rpart_fit_neqp, testing_sen) 

# > Performs similar to training set
# > As seen on training set, Mbeki speeches are predicted more IF Mbeki (and v.v.)

```

## SECTION 3: Exploratory analysis

### a. Plot: Most commonly used words/bigrams (president comparison)

#### i. Single words

Absolute

```{r, echo = F, eval = T, fig.cap = "Most commonly used words"}

library(drlib)

tidy_word %>% 
  filter(!word %in% stop_words$word) %>%
  group_by(president_id) %>%
  count(word, sort = TRUE) %>%
  top_n(10, n) %>%
  ungroup() %>% 
  mutate(word = reorder_within(word, n, president_id)) %>%
  ggplot(aes(word, n, fill = president_id)) + geom_col(show.legend = F) + 
  scale_x_reoredered() + 
  coord_flip() + facet_wrap(~ president_id, scales = "free_y") + labs(x = "")

# > Remove stop words AS sentence construction words are non-informative
# > Absolute word frequency is misleading >> different number of speeches
# > Words:
#   - Government (Mbeki) 
#   - All (Mandela and Mbeki)

```

Relative

```{r}

total_speeches <- tidy_word %>% 
  group_by(president_id) %>% 
  summarise(total = n())

tidy_word %>%
  filter(!word %in% stop_words$word) %>%
  group_by(president_id) %>% 
  count(word, sort = TRUE) %>%                              # Count: Number of times word used 
  left_join(total_speeches, by = "president_id") %>%        # Add: Total speeches for each president
  mutate(freq = n/total) %>%                                # Add relative frequencies
  filter(rank(desc(freq)) < 20) %>%
  ggplot(aes(reorder(word, freq), freq, fill = president_id)) + 
  geom_col() + 
  coord_flip() + 
    xlab("") +
  facet_grid(.~ president_id)

# Klerk has a reltively high frequency of mentioning the constitution 
# > development of the constitution pre-1994 election
# Government mentioned by Mbeki (as seen above)
# Pronouns indicative of atmosphere: Relative "we" frequency of Ramaphosa suggest he is trying to unite the country
# Sentence counstruction words are prevalent 

```

Weighted by inverse term frquency

```{r}

tidy_word %>% 
  left_join(select(sentence_tdf, sen_id, word, tf_idf), by = c("sen_id", "word")) %>%
  group_by(president_id, word) %>% 
  filter(rank(desc(tf_idf)) < 5) %>%
  ggplot(aes(reorder(word, tf_idf), tf_idf, fill = president_id)) + 
  geom_col() + 
  coord_flip() + 
  xlab("")

```

#### ii. Bigrams

```{r}

tidy_bigrams <- tidy_sen %>% 
  unnest_tokens(bigram, sentences, token = "ngrams", n = 2)

# a. Separate the bigrams 

bigrams_separated <- tidy_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# b. Remove stop words

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# c. Rejoin bigrams

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# Plot: Most common bigrams 

bigrams_united %>% 
  group_by(president_id) %>%
  count(bigram, sort = TRUE) %>% 
  left_join(total_speeches) %>%                             # Add: Total speeches for each president
  mutate(freq = n/total) %>%                                # Add relative frequencies
  filter(rank(desc(freq)) < 10) %>%
  ggplot(aes(reorder(bigram, freq), freq, fill = president_id)) + 
  geom_col() + 
  coord_flip() + 
    xlab("") +
  facet_grid(.~president_id)

# All presidents: South Africa/n focussed
# De Klerk >> Focussed on own party and constitution/structure of future government
# Mandela >> BEE, human rights, job creation (Priorities of gov.)
# Mbeki >> 

```

### b. Data properties

```{r}

## Time: First, last AND time range of speech data

min(speeches$date) # Earliest
max(speeches$date) # Latest

max(speeches$date) - min(speeches$date) # Time range

# Plot:

ggplot(speeches, aes(x = date, fill = president_id)) +
  geom_histogram(position = "identity", 
                 bins = 30, 
                 show.legend = T) + 
  labs(x = "Date", 
       y = "Count") 

## Number of speeches by each president 

table(speeches$president)
                
```






















